{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 🎲 **Sampling** – *Control Randomness in LLM Responses*\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 What It Does\n",
    "\n",
    "**Sampling** lets you fine-tune the **creativity vs. consistency** of LLM-generated outputs using parameters like `temperature`, `top_p`, and `max_tokens`.\n",
    "\n",
    "It’s useful when you want to **balance creativity, reliability, and diversity** in completions across different use-cases.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Common Use-Cases\n",
    "\n",
    "| Scenario               | Why Use It                                        |\n",
    "| ---------------------- | ------------------------------------------------- |\n",
    "| ✍️ Creative writing    | Use higher randomness for stories, poems          |\n",
    "| 🤖 Chatbots            | Balanced sampling ensures varied but safe replies |\n",
    "| 🧠 Brainstorming ideas | Generates diverse concepts using higher sampling  |\n",
    "| ✅ Data extraction      | Use low randomness for repeatable structure       |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Key Parameters\n",
    "\n",
    "| Parameter     | Type    | Description                                                               |\n",
    "| ------------- | ------- | ------------------------------------------------------------------------- |\n",
    "| `temperature` | `float` | Controls randomness: lower = consistent, higher = more creative (0.2–1.0) |\n",
    "| `top_p`       | `float` | Nucleus sampling: only top % of tokens are considered (e.g. 0.9)          |\n",
    "| `max_tokens`  | `int`   | Limits length of output to avoid runaway generations                      |\n",
    "| `stop`        | `list`  | List of tokens where generation should halt (e.g., \\[\"\\n\", \"###\"])        |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎛️ When to Adjust\n",
    "\n",
    "| Use-Case                 | Recommended Settings                     |\n",
    "| ------------------------ | ---------------------------------------- |\n",
    "| ✅ Deterministic task     | `temperature=0.1`, `top_p=1.0`           |\n",
    "| 💡 Brainstorm ideas      | `temperature=0.8+`, `top_p=0.9`          |\n",
    "| 📄 Structured outputs    | `temperature=0.3`, include stop tokens   |\n",
    "| 🤹 Conversational agents | `temperature=0.6–0.9`, `top_p=0.85–0.95` |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Sample Usage\n",
    "\n",
    "```python\n",
    "from mcp.completion import complete\n",
    "\n",
    "complete(\n",
    "    prompt=\"Suggest three fun activities in Tokyo for a weekend trip\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    max_tokens=100\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Tip: Deterministic vs. Creative\n",
    "\n",
    "| Mode            | Behavior                        |\n",
    "| --------------- | ------------------------------- |\n",
    "| `temperature=0` | Always same output              |\n",
    "| `temperature=1` | Random, exploratory answers     |\n",
    "| `top_p=1.0`     | All possible tokens considered  |\n",
    "| `top_p=0.8`     | Top 80% probability tokens only |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "| Feature            | Description                                    |\n",
    "| ------------------ | ---------------------------------------------- |\n",
    "| Control randomness | Tune creativity with temperature and top\\_p    |\n",
    "| Safer outputs      | Use stop tokens and max\\_tokens                |\n",
    "| Versatile usage    | Creative, chat, data extraction, summarization |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
