{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ² **Sampling** â€“ *Control Randomness in LLM Responses*\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ What It Does\n",
    "\n",
    "**Sampling** lets you fine-tune the **creativity vs. consistency** of LLM-generated outputs using parameters like `temperature`, `top_p`, and `max_tokens`.\n",
    "\n",
    "Itâ€™s useful when you want to **balance creativity, reliability, and diversity** in completions across different use-cases.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Common Use-Cases\n",
    "\n",
    "| Scenario               | Why Use It                                        |\n",
    "| ---------------------- | ------------------------------------------------- |\n",
    "| âœï¸ Creative writing    | Use higher randomness for stories, poems          |\n",
    "| ğŸ¤– Chatbots            | Balanced sampling ensures varied but safe replies |\n",
    "| ğŸ§  Brainstorming ideas | Generates diverse concepts using higher sampling  |\n",
    "| âœ… Data extraction      | Use low randomness for repeatable structure       |\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Key Parameters\n",
    "\n",
    "| Parameter     | Type    | Description                                                               |\n",
    "| ------------- | ------- | ------------------------------------------------------------------------- |\n",
    "| `temperature` | `float` | Controls randomness: lower = consistent, higher = more creative (0.2â€“1.0) |\n",
    "| `top_p`       | `float` | Nucleus sampling: only top % of tokens are considered (e.g. 0.9)          |\n",
    "| `max_tokens`  | `int`   | Limits length of output to avoid runaway generations                      |\n",
    "| `stop`        | `list`  | List of tokens where generation should halt (e.g., \\[\"\\n\", \"###\"])        |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›ï¸ When to Adjust\n",
    "\n",
    "| Use-Case                 | Recommended Settings                     |\n",
    "| ------------------------ | ---------------------------------------- |\n",
    "| âœ… Deterministic task     | `temperature=0.1`, `top_p=1.0`           |\n",
    "| ğŸ’¡ Brainstorm ideas      | `temperature=0.8+`, `top_p=0.9`          |\n",
    "| ğŸ“„ Structured outputs    | `temperature=0.3`, include stop tokens   |\n",
    "| ğŸ¤¹ Conversational agents | `temperature=0.6â€“0.9`, `top_p=0.85â€“0.95` |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Sample Usage\n",
    "\n",
    "```python\n",
    "from mcp.completion import complete\n",
    "\n",
    "complete(\n",
    "    prompt=\"Suggest three fun activities in Tokyo for a weekend trip\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    max_tokens=100\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Tip: Deterministic vs. Creative\n",
    "\n",
    "| Mode            | Behavior                        |\n",
    "| --------------- | ------------------------------- |\n",
    "| `temperature=0` | Always same output              |\n",
    "| `temperature=1` | Random, exploratory answers     |\n",
    "| `top_p=1.0`     | All possible tokens considered  |\n",
    "| `top_p=0.8`     | Top 80% probability tokens only |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "| Feature            | Description                                    |\n",
    "| ------------------ | ---------------------------------------------- |\n",
    "| Control randomness | Tune creativity with temperature and top\\_p    |\n",
    "| Safer outputs      | Use stop tokens and max\\_tokens                |\n",
    "| Versatile usage    | Creative, chat, data extraction, summarization |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
